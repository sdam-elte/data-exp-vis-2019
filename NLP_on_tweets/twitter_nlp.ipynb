{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Very Brief Intro to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fastest way to find patterns in strings is to use a library that can match so-called regular expressions. In Python, it is the `re` library that does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is only a really simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'pear', 'plum', 'cherries']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+','apple pear plum cherries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quite good introduction:\n",
    "http://www.zytrax.com/tech/web/regex.htm\n",
    "\n",
    "This is a really good tutorial that shows how regular expressions match or do not match:\n",
    "https://regexone.com/\n",
    "\n",
    "And this link covers Pythons regex library as a starter.\n",
    "https://developers.google.com/edu/python/regular-expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to feed a text to the computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we want to decide what elements we want as building blocks. A common approach is to break the text down to words. But even this task is not that simple as it first seems. The next string contains an example message of Donald Trump from the Twitter social network.\n",
    "\n",
    "Possible problems:\n",
    "* lower and uppercase\n",
    "* punctuation\n",
    "* emojis\n",
    "* character encoding\n",
    "* named entity recognition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweet = 'Having great meetings and discussions with my friend, President @EmmanuelMacron of France. We are in the midst of meetings on Iran, Syria and Trade. We will be holding a joint press conference shortly, here at the @WhiteHouse. ðŸ‡ºðŸ‡¸ðŸ‡«ðŸ‡·'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having great meetings and discussions with my friend president emmanuelmacron of france we are in the midst of meetings on iran syria and trade we will be holding a joint press conference shortly here at the whitehouse\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(re.findall('\\w+',trump_tweet.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bokanyie/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our former task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having great meetings and discussions with my friend , president @ emmanuelmacron of france . we are in the midst of meetings on iran , syria and trade . we will be holding a joint press conference shortly , here at the @ whitehouse . ðŸ‡ºðŸ‡¸ðŸ‡«ðŸ‡·\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(nltk.word_tokenize(trump_tweet.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for a good tweet tokenizer that has Twitter-specific regular expressions built-in either within `nltk` or outside!\n",
    "http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in the core meaning, often we can throw away third person forms or plurals etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having -> having\n",
      "great -> great\n",
      "meetings -> meet\n",
      "and -> and\n",
      "discussions -> discuss\n",
      "with -> with\n",
      "my -> my\n",
      "friend -> friend\n",
      ", -> ,\n",
      "president -> presid\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow = SnowballStemmer('english',ignore_stopwords=True)\n",
    "for word in nltk.word_tokenize(trump_tweet.lower())[0:10]:\n",
    "    print(word,'->',snow.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful algebraic concept for representing a collection of texts.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Document-term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at such a matrix on some Trump tweets again http://www.trumptwitterarchive.com/archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "obamacare = pd.read_csv('obamacare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 3, 0, 1,\n",
       "         0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "         0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 4, 1, 0,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.fit_transform(obamacare['text'].head(3).tolist()).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00 amp and approved approximately as at based be been being biggest bill come conference cut democrats develop essentially eventually fact final for goes great has hated healthcare history house if in individual is just mandate morning most new news obamacare of on our over part passed plan reform remember repealed repeals republicans senate signed states tax terminated terrible that the there time to together tomorrow under unfair united unpopular very vote which white will'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(c.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It excluded so-called stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other questions\n",
    "\n",
    "What to do when the texts do not have the same length? How to exclude not relevant words? How to make a weighting scheme for our purposes?\n",
    "\n",
    "How to deal with the 'timeseries' nature of texts? What inputs do neural networks expect?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
